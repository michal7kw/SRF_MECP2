{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import concurrent.futures  # Add this import\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import logging\n",
    "from multiprocessing import Pool, Lock, current_process\n",
    "from functools import partial\n",
    "import sys\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current working directory\n",
    "os.chdir('/beegfs/scratch/ric.broccoli/kubacki.michal/SRF_MECP2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Decompress the GTF file\n",
    "with gzip.open('gencode.vM10.annotation.gtf.gz', 'rb') as f_in:\n",
    "    with open('gencode.vM10.annotation.gtf', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_deseq_results(deseq_file, padj_threshold=0.05, lfc_threshold=1):\n",
    "    \"\"\"\n",
    "    Process DESeq2 results using vectorized operations\n",
    "    \"\"\"\n",
    "    # Read the entire file - we'll use vectorized operations for speed\n",
    "    df = pd.read_csv(deseq_file)\n",
    "    \n",
    "    # Use vectorized operations for categorization\n",
    "    up_mask = (df['padj'] < padj_threshold) & (df['log2FoldChange'] > lfc_threshold)\n",
    "    down_mask = (df['padj'] < padj_threshold) & (df['log2FoldChange'] < -lfc_threshold)\n",
    "    non_mask = ~(up_mask | down_mask)\n",
    "    \n",
    "    return {\n",
    "        'up': df.loc[up_mask, 'gene'].tolist(),\n",
    "        'down': df.loc[down_mask, 'gene'].tolist(),\n",
    "        'non': df.loc[non_mask, 'gene'].tolist()\n",
    "    }\n",
    "\n",
    "def create_bed_file(args):\n",
    "    \"\"\"\n",
    "    Worker function for parallel BED file creation\n",
    "    \"\"\"\n",
    "    genes, gtf_file, output_file, category = args\n",
    "    \n",
    "    # Create a temporary file with gene list\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:\n",
    "        temp_file.write('\\n'.join(genes))\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    try:\n",
    "        # Use grep for faster filtering and awk for processing\n",
    "        cmd = f\"\"\"grep -F -f {temp_file_path} {gtf_file} | \n",
    "                 awk -F'\\t' '$3==\"gene\" {{\n",
    "                     if (match($9, /gene_name \"([^\"]+)\"/, n))\n",
    "                         print $1\"\\t\"$4-1\"\\t\"$5\"\\t\"n[1]\"\\t.\\t\"$7\n",
    "                 }}' > {output_file}\"\"\"\n",
    "        \n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        os.unlink(temp_file_path)\n",
    "    \n",
    "    return output_file, category\n",
    "\n",
    "def process_tissue_data(args):\n",
    "    \"\"\"\n",
    "    Process all data for a single tissue\n",
    "    \"\"\"\n",
    "    tissue, deseq_file, gtf_file, output_dir = args\n",
    "    \n",
    "    # Process DESeq2 results\n",
    "    gene_sets = process_deseq_results(deseq_file)\n",
    "    \n",
    "    # Prepare output directory\n",
    "    tissue_output_dir = os.path.join(output_dir, tissue)\n",
    "    os.makedirs(tissue_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create bed files\n",
    "    bed_files = {}\n",
    "    for category, genes in gene_sets.items():\n",
    "        output_file = f\"{tissue_output_dir}/{tissue}_{category}_regulated_genes.bed\"\n",
    "        bed_files[category] = output_file\n",
    "        \n",
    "        # Create the bed file directly (no parallelization here)\n",
    "        create_bed_file((genes, gtf_file, output_file, category))\n",
    "    \n",
    "    return tissue, gene_sets, bed_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "deseq_files = {\n",
    "    'Neuron': 'DEA_NEU.csv',\n",
    "    'NSC': 'DEA_NSC.csv'\n",
    "}\n",
    "gtf_file = \"gencode.vM10.annotation.gtf\"  \n",
    "output_dir = \"metaprofile_results\"\n",
    "sample_sheet = \"EXOGENOUS_sample_sheet.csv\"\n",
    "genome_size_file = \"mm10.chrom.sizes\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prepare arguments for parallel processing\n",
    "process_args = [\n",
    "    (tissue, deseq_file, gtf_file, output_dir)\n",
    "    for tissue, deseq_file in deseq_files.items()\n",
    "]\n",
    "\n",
    "# Process tissues in parallel (single level of parallelization)\n",
    "print(f\"Processing {len(deseq_files)} tissues using {min(cpu_count(), len(deseq_files))} processes...\")\n",
    "\n",
    "with Pool(processes=min(cpu_count(), len(deseq_files))) as pool:\n",
    "    results = pool.map(process_tissue_data, process_args)\n",
    "\n",
    "# Organize results\n",
    "tissue_gene_sets = {}\n",
    "tissue_bed_files = {}\n",
    "\n",
    "for tissue, gene_sets, bed_files in results:\n",
    "    tissue_gene_sets[tissue] = gene_sets\n",
    "    tissue_bed_files[tissue] = bed_files\n",
    "    print(f\"Completed processing {tissue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Neuron')\n",
    "for key in tissue_gene_sets['Neuron'].keys():\n",
    "    print(key)\n",
    "    print(len(tissue_gene_sets['Neuron'][key]))\n",
    "\n",
    "print('\\nNSC')\n",
    "for key in tissue_gene_sets['NSC'].keys():\n",
    "    print(key)\n",
    "    print(len(tissue_gene_sets['NSC'][key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct awk command to extract coordinates from GTF:\n",
    "# -F'\\t' : Set tab as field separator\n",
    "# $3==\"gene\" : Only process lines where 3rd column is \"gene\"\n",
    "# match($9, /gene_name \"([^\"]+)\"/, n) : Extract gene name from 9th column attributes\n",
    "# n[1]~/{gene_list}/ : Check if extracted gene name matches our gene list\n",
    "# Print format: chromosome start end gene_name score strand\n",
    "# Note: BED format is 0-based, so we subtract 1 from start coordinate ($4-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_metaprofiles(bed_files, sample_sheet, output_dir):\n",
    "#     \"\"\"\n",
    "#     Generate metaprofiles using deepTools\n",
    "#     \"\"\"\n",
    "#     # Read sample sheet\n",
    "#     samples = pd.read_csv(sample_sheet)\n",
    "    \n",
    "#     for tissue in ['Neuron', 'NSC']:\n",
    "#         tissue_samples = samples[samples['Tissue'] == tissue]\n",
    "#         bam_files = tissue_samples['bamReads'].tolist()\n",
    "        \n",
    "#         for category, bed_file in bed_files.items():\n",
    "#             # Generate matrix using computeMatrix\n",
    "#             matrix_file = f\"{output_dir}/{tissue}_{category}_matrix.gz\"\n",
    "#             profile_file = f\"{output_dir}/{tissue}_{category}_profile.png\"\n",
    "            \n",
    "#             cmd_matrix = f\"\"\"\n",
    "#             computeMatrix scale-regions -S {' '.join(bam_files)} \\\n",
    "#                 -R {bed_file} \\\n",
    "#                 -b 5000 -a 5000 \\\n",
    "#                 --regionBodyLength 5000 \\\n",
    "#                 --skipZeros \\\n",
    "#                 -o {matrix_file}\n",
    "#             \"\"\"\n",
    "            \n",
    "#             cmd_plot = f\"\"\"\n",
    "#             plotProfile -m {matrix_file} \\\n",
    "#                 --perGroup \\\n",
    "#                 --colors red blue \\\n",
    "#                 --plotTitle \"{tissue} {category}-regulated genes\" \\\n",
    "#                 -out {profile_file}\n",
    "#             \"\"\"\n",
    "            \n",
    "#             subprocess.run(cmd_matrix, shell=True)\n",
    "#             subprocess.run(cmd_plot, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bam_to_bigwig(bam_file, output_dir, genome_size_file):\n",
    "    \"\"\"\n",
    "    Convert BAM file to bigWig format using deeptools\n",
    "    \"\"\"\n",
    "    basename = os.path.splitext(os.path.basename(bam_file))[0]\n",
    "    output_bw = f\"{output_dir}/{basename}.bw\"\n",
    "    \n",
    "    if not os.path.exists(output_bw):\n",
    "        print(f\"Converting {basename} BAM to bigWig...\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create unique temporary directory for this conversion\n",
    "        with tempfile.TemporaryDirectory(prefix=f\"tmp_{basename}_\") as temp_dir:\n",
    "            # Sort and index BAM\n",
    "            print(f\"- Sorting and indexing {basename} BAM...\")\n",
    "            sorted_bam = f\"{temp_dir}/{basename}.sorted.bam\"\n",
    "            subprocess.run(f\"samtools sort -T {temp_dir}/sort_tmp {bam_file} -o {sorted_bam}\", \n",
    "                         shell=True, check=True)\n",
    "            subprocess.run(f\"samtools index {sorted_bam}\", shell=True, check=True)\n",
    "            \n",
    "            # Convert directly to bigWig using bamCoverage\n",
    "            print(f\"- Converting {basename} to bigWig...\")\n",
    "            subprocess.run(f\"\"\"\n",
    "                bamCoverage -b {sorted_bam} \\\n",
    "                    -o {output_bw} \\\n",
    "                    --binSize 10 \\\n",
    "                    --normalizeUsing RPKM \\\n",
    "                    --numberOfProcessors 1\n",
    "            \"\"\", shell=True, check=True)\n",
    "            \n",
    "            print(f\"Completed conversion of {basename} to bigWig\")\n",
    "    else:\n",
    "        print(f\"BigWig file already exists for {basename}, skipping conversion\")\n",
    "    \n",
    "    return output_bw\n",
    "\n",
    "def generate_metaprofiles(tissue_bed_files, sample_sheet, output_dir, genome_size_file):\n",
    "    \"\"\"\n",
    "    Generate metaprofiles using deepTools with parallel processing\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting metaprofile generation...\")\n",
    "    \n",
    "    # Read sample sheet\n",
    "    print(\"Reading sample sheet...\")\n",
    "    samples = pd.read_csv(sample_sheet)\n",
    "    bigwig_dir = f\"{output_dir}/bigwig\"\n",
    "    os.makedirs(bigwig_dir, exist_ok=True)\n",
    "    \n",
    "    # Process BAM files sequentially to avoid resource conflicts\n",
    "    print(\"\\nConverting BAM files to bigWig format...\")\n",
    "    bigwig_files = []\n",
    "    for bam_file in samples['bamReads'].tolist():\n",
    "        try:\n",
    "            bigwig_file = bam_to_bigwig(bam_file, bigwig_dir, genome_size_file)\n",
    "            bigwig_files.append(bigwig_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {bam_file}: {str(e)}\")\n",
    "            return\n",
    "    \n",
    "    samples['bigWig'] = bigwig_files\n",
    "    \n",
    "    # Create a list of all tasks to run\n",
    "    print(\"\\nPreparing metaprofile generation tasks...\")\n",
    "    tasks = []\n",
    "    for tissue in ['Neuron', 'NSC']:\n",
    "        tissue_samples = samples[samples['Tissue'] == tissue]\n",
    "        tissue_bigwigs = tissue_samples['bigWig'].tolist()\n",
    "        bed_files = tissue_bed_files[tissue]\n",
    "        \n",
    "        for category, bed_file in bed_files.items():\n",
    "            # Create unique temporary directory for matrix files\n",
    "            matrix_dir = f\"{output_dir}/matrix_{tissue}_{category}\"\n",
    "            os.makedirs(matrix_dir, exist_ok=True)\n",
    "            \n",
    "            matrix_file = f\"{matrix_dir}/{tissue}_{category}_matrix.gz\"\n",
    "            profile_file = f\"{output_dir}/{tissue}_{category}_profile.png\"\n",
    "            \n",
    "            cmd_matrix = f\"\"\"\n",
    "            computeMatrix scale-regions -S {' '.join(tissue_bigwigs)} \\\n",
    "                -R {bed_file} \\\n",
    "                -b 5000 -a 5000 \\\n",
    "                --regionBodyLength 5000 \\\n",
    "                --skipZeros \\\n",
    "                --numberOfProcessors 1 \\\n",
    "                -o {matrix_file}\n",
    "            \"\"\"\n",
    "            \n",
    "            cmd_plot = f\"\"\"\n",
    "            plotProfile -m {matrix_file} \\\n",
    "                --perGroup \\\n",
    "                --colors red blue \\\n",
    "                --plotTitle \"{tissue} {category}-regulated genes\" \\\n",
    "                -out {profile_file}\n",
    "            \"\"\"\n",
    "            \n",
    "            tasks.append((tissue, category, cmd_matrix, cmd_plot))\n",
    "\n",
    "    # Process tasks sequentially to avoid resource conflicts\n",
    "    print(f\"\\nExecuting {len(tasks)} metaprofile tasks...\")\n",
    "    for task in tasks:\n",
    "        tissue, category, cmd_matrix, cmd_plot = task\n",
    "        try:\n",
    "            print(f\"\\nProcessing {tissue} {category}-regulated genes...\")\n",
    "            print(\"- Generating matrix...\")\n",
    "            subprocess.run(cmd_matrix, shell=True, check=True)\n",
    "            print(\"- Creating profile plot...\")\n",
    "            subprocess.run(cmd_plot, shell=True, check=True)\n",
    "            print(f\"Completed processing {tissue} {category}-regulated genes\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error processing {tissue} {category}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nMetaprofile generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metaprofiles\n",
    "generate_metaprofiles(tissue_bed_files, sample_sheet, output_dir, genome_size_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_profiles(output_dir):\n",
    "    \"\"\"\n",
    "    Create combined visualization\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        categories = ['non', 'up', 'down']\n",
    "        tissues = ['Neuron', 'NSC']\n",
    "        \n",
    "        for i, tissue in enumerate(tissues):\n",
    "            for j, category in enumerate(categories):\n",
    "                matrix_file = f\"{output_dir}/{tissue}_{category}_matrix.gz\"\n",
    "                if os.path.exists(matrix_file):\n",
    "                    data = np.loadtxt(matrix_file)\n",
    "                    \n",
    "                    x = np.linspace(-5000, 5000, data.shape[1])\n",
    "                    mean_profile = data.mean(axis=0)\n",
    "                    std_profile = data.std(axis=0)\n",
    "                    \n",
    "                    axes[i, j].plot(x, mean_profile)\n",
    "                    axes[i, j].fill_between(x, \n",
    "                                          mean_profile - std_profile,\n",
    "                                          mean_profile + std_profile,\n",
    "                                          alpha=0.2)\n",
    "                    \n",
    "                    axes[i, j].set_title(f\"{tissue} {category}-regulated\")\n",
    "                    axes[i, j].set_xlabel(\"Distance from TSS (bp)\")\n",
    "                    axes[i, j].set_ylabel(\"Average signal\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/combined_profiles.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating combined plot: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined visualization\n",
    "plot_combined_profiles(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
